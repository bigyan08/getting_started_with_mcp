{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7e2360",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from dotenv import load_dotenv\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.agents import create_agent\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "from langchain.agents.middleware import dynamic_prompt, ModelRequest\n",
    "import tqdm \n",
    "import json\n",
    "import getpass\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c05ff54",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'path/to/dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0c02e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.environ.get(\"GOOGLE_API_KEY\"):\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")\n",
    "\n",
    "llm = init_chat_model(\"google_genai:gemini-2.5-flash-lite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592da040",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HuggingFaceEmbeddings(\n",
    "    model_name=\"all-MiniLM-L6-v2\",\n",
    "    model_kwargs={'device': 'cuda'}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a7e77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "ids = []\n",
    "\n",
    "with open(PATH) as f:\n",
    "    for line in f:\n",
    "        if line.strip():\n",
    "            entry = json.loads(line)\n",
    "            text = \" \".join(\"Title: \",entry.get([\"title\",\"\"]),\"\\nAbstract: \",entry.get([\"abstract\",\"\"]))\n",
    "            texts.append(text)\n",
    "            ids.append(entry.get(\"id\",\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c484465b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = FAISS.from_texts(\n",
    "    texts = texts,  # Used only 500k(texts[:500000]) of research papers due to computational issue, can do 2.8M papers.\n",
    "    embedding = model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553fdf02",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dynamic_prompt\n",
    "def prompt_with_context(request: ModelRequest) -> str:\n",
    "    \"\"\"Inject context into state messages.\"\"\"\n",
    "    last_query = request.state[\"messages\"][-1].text\n",
    "    retrieved_docs = vector_store.similarity_search(last_query)\n",
    "\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
    "\n",
    "    system_message = (\n",
    "        \"You are a Research Assistant with access to the user's personal notes and documents. Always search the knowledge base first before answering questions, and cite which documents your information comes from (e.g., 'According to [Document Name], ...'). If information isn't in the knowledge base, clearly state 'I don't find information about this in your knowledge base' and offer to use general knowledge instead. When answering, combine information from multiple sources when relevant, keep responses focused on what's most relevant to the question, and never fabricate citations or attribute information to documents it doesn't come from. Your goal is to help users extract maximum value from their stored knowledge by retrieving and synthesizing information clearly. Use the following context in your response:\"\n",
    "        f\"\\n\\n{docs_content}\"\n",
    "    )\n",
    "\n",
    "    return system_message\n",
    "\n",
    "\n",
    "agent = create_agent(llm, tools=[], middleware=[prompt_with_context])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4c620d",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is prompt diphoton?\"\n",
    "for step in agent.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    step[\"messages\"][-1].pretty_print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

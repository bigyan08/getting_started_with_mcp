{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81ce2dbd",
   "metadata": {},
   "source": [
    "## Selecting the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ca4d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "model = init_chat_model(\"google_genai:gemini-2.5-flash-lite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11322bb",
   "metadata": {},
   "source": [
    "## Selecting the Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6f5d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"GOOGLE_API_KEY\"):\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")\n",
    "\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-001\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2caf9bb",
   "metadata": {},
   "source": [
    "## Selecting the Memory (Chroma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f02514",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"example_collection\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"./chroma_langchain_db\",  # Where to save data locally, remove if not necessary\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f8dc11",
   "metadata": {},
   "source": [
    "## 1. Indexing Process:\n",
    "1. Loading different documents\n",
    "2. Splitting the documents into chunks\n",
    "3. Creating embeddings for the chunks\n",
    "4. Storing the embeddings in a vector database (Chroma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b0d38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "# Only keep post title, headers, and content from the full HTML.\n",
    "bs4_strainer = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs={\"parse_only\": bs4_strainer},\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "assert len(docs) == 1\n",
    "print(f\"Total characters: {len(docs[0].page_content)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31581cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3119532",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[0].page_content[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc92a3eb",
   "metadata": {},
   "source": [
    "### Splitting the Documents into Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a22fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Recursive text splitter works by recursively splitting the text based on the commonseparators like new line until all the chunks are appropriate size.\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,  # chunk size (characters)\n",
    "    chunk_overlap=200,  # chunk overlap (characters)\n",
    "    add_start_index=True,  # track index in original document\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "print(f\"Split blog post into {len(all_splits)} sub-documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadebced",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_splits[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ef5631",
   "metadata": {},
   "source": [
    "### Vector Store\n",
    "Storing the splitted document chunks into a vector database for efficient retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a4f263",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_ids = vector_store.add_documents(documents=all_splits)\n",
    "\n",
    "print(document_ids[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6b8ef7",
   "metadata": {},
   "source": [
    "## 2. Retrieval and Generation Process:\n",
    "1. Retrieve: Given a user input, relevant splits are retrieved from storage using a Retriever.\n",
    "2. Generate: A model produces an answer using a prompt that includes both the question with the retrieved data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46db3ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "\n",
    "@tool(response_format=\"content_and_artifact\")\n",
    "def retrieve_context(query: str):\n",
    "    \"\"\"Retrieve information to help answer a query.\"\"\"\n",
    "    retrieved_docs = vector_store.similarity_search(query, k=2)\n",
    "    serialized = \"\\n\\n\".join(\n",
    "        (f\"Source: {doc.metadata}\\nContent: {doc.page_content}\")\n",
    "        for doc in retrieved_docs\n",
    "    )\n",
    "    return serialized, retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f85068",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "\n",
    "tools = [retrieve_context]\n",
    "# If desired, specify custom instructions\n",
    "prompt = (\n",
    "    \"You have access to a tool that retrieves context from a blog post. \"\n",
    "    \"Use the tool to help answer user queries.\"\n",
    ")\n",
    "agent = create_agent(model, tools, system_prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6c84e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = (\n",
    "    \"What is the standard method for Task Decomposition?\"\n",
    ")\n",
    "\n",
    "for event in agent.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c65c0b",
   "metadata": {},
   "source": [
    "## OR,\n",
    "create a chain using dynamic prompting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45873656",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.middleware import dynamic_prompt, ModelRequest\n",
    "\n",
    "@dynamic_prompt\n",
    "def prompt_with_context(request: ModelRequest) -> str:\n",
    "    \"\"\"Inject context into state messages.\"\"\"\n",
    "    last_query = request.state[\"messages\"][-1].text\n",
    "    retrieved_docs = vector_store.similarity_search(last_query)\n",
    "\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
    "\n",
    "    system_message = (\n",
    "        \"You are a helpful assistant. Use the following context in your response:\"\n",
    "        f\"\\n\\n{docs_content}\"\n",
    "    )\n",
    "\n",
    "    return system_message\n",
    "\n",
    "agent = create_agent(model, tools=[], middleware=[prompt_with_context])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f07685",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is task decomposition?\"\n",
    "for step in agent.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    step[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496a6793",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

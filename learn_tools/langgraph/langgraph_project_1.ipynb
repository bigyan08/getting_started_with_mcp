{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f4bbfb9",
   "metadata": {},
   "source": [
    "## Version 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accc3c6b",
   "metadata": {},
   "source": [
    "Traditional Agents feel a lot like following a recipe. You go step by step in the same order every time: take the input, apply the logic, call the tool, give the output. It works well when the task is predictable, but it struggles the moment anything unexpected happens.\n",
    "\n",
    "ReAct Agents (Reason + Act) behave more like how we handle real situations. They think, take an action, look at what happened, and rethink their plan if needed. They constantly check whether they should call a tool or just move on, and they improve their decision as new information comes in. This makes them much more flexible and much better at multi-step or messy tasks. React follows reason->act->observe cycle.\n",
    "\n",
    "\n",
    "The core idea is simple:\n",
    "Traditional Agents follow a straight line.\n",
    "ReAct Agents follow a loop that learns from each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67cdac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated\n",
    "from langgraph.graph.message import add_messages\n",
    "class AgentState(TypedDict):\n",
    "    ''' add_messages is used to store the messages properly since the property of State is to update and remove previous values.\n",
    "        add_messages help store those changes.'''\n",
    "    messages : Annotated[list, add_messages]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c41c56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = init_chat_model(\"google_genai:gemini-2.5-flash-lite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5c805d",
   "metadata": {},
   "source": [
    "agentnode will act the llm brain of the agent. Takes in the messages and produces responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ad527d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent_node(state:AgentState):\n",
    "    '''\n",
    "    Reads the messages -> Passes those into llm -> updates the state with the response from llm\n",
    "    '''\n",
    "    messages = state[\"messages\"]\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"messages\":response}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3356bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph , START, END\n",
    "\n",
    "builder = StateGraph(AgentState)\n",
    "\n",
    "# builder.add_node(\"agent\",agent_node)\n",
    "# builder.add_edge(START,\"agent\")\n",
    "# builder.add_edge(\"agent\",END)\n",
    "# graph = builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d02e102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# out =graph.invoke({\n",
    "#     \"messages\": ['hello','how are you']\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b83f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0428a9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import ToolNode\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "# # temporary: no real tools, empty list\n",
    "# tools = []\n",
    "\n",
    "# tool_node = ToolNode(tools)\n",
    "\n",
    "# builder.add_node(\"tools\", tool_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec97489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# builder.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee25d22",
   "metadata": {},
   "source": [
    "Now we will use a conditional edge to check if the agent is calling tool or not. If tool is being called, we will use tool node to call the tool and update the messages in the state. If not, we will return the final response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6c43b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_continue(state:AgentState):\n",
    "    '''\n",
    "    Check the last message -> if llm wants to call tools then return 'tools' else dont call\n",
    "    '''\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    # langchain messages usually have .tool_calls when the llm wants tools\n",
    "    if last_message.tool_calls:\n",
    "        return 'tools'\n",
    "    return END\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b36ea31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# builder.add_conditional_edges(\n",
    "#     \"agent\",\n",
    "#     should_continue,\n",
    "#     [\"tools\",END]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3528e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# builder.add_edge(\"tools\",\"agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab444058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# builder.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db7c253",
   "metadata": {},
   "source": [
    "## Version 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11b63b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_tavily import TavilySearch\n",
    "\n",
    "search_tool = TavilySearch(max_results=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609af26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "out=search_tool.invoke(\"what is langchain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79e7eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0a7a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf3356d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#updating tools list\n",
    "\n",
    "tools = [search_tool]\n",
    "tool_node = ToolNode(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0d7b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "def agent_node(state: AgentState):\n",
    "    messages = state['messages']\n",
    "    response = llm_with_tools.invoke(messages)\n",
    "    return {'messages':[response]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fc8037",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def save_note(content:str,tags:list[str]):\n",
    "    '''\n",
    "    Save the note to a file. This is done after getting response from the LLM.\n",
    "    '''\n",
    "    filename=\"research_notes.txt\"\n",
    "    with open(filename,'a') as f:\n",
    "        f.write(f\"\\n--- NOTE ---\\n\")\n",
    "        f.write(f\"TAGS: {', '.join(tags)}\\n\")\n",
    "        f.write(f\"CONTENT: {content}\\n\")\n",
    "        f.write(f\"------------\\n\")\n",
    "    return f\"Note saved to {filename}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f77da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#update the tools node\n",
    "tools = [search_tool,save_note]\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "tool_node =ToolNode(tools) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945dc9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "builder.add_node(\"agent\",agent_node)\n",
    "builder.add_node(\"tools\",tool_node)\n",
    "builder.add_edge(START,\"agent\")\n",
    "builder.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    should_continue,\n",
    "    [\"tools\",END]\n",
    ")\n",
    "builder.add_edge(\"tools\",\"agent\")\n",
    "graph = builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d006a9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e33cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_state = {\n",
    "    \"messages\" :[(\"user\",\"Research 'Langchain' and save a short summary with tags 'AI', 'Python'.\")]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacc0cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "for event in graph.stream(input=initial_state):\n",
    "    for key,value in event.items():\n",
    "        try:\n",
    "            print(f\"\\nNode {key} finished.\")\n",
    "            print(value[\"messages\"][-1].content)\n",
    "        except Exception as e:\n",
    "            print(f\"{e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce894a81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
